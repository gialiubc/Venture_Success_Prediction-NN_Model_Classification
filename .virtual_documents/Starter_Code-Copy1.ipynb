


# Import our dependencies
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt
import hvplot.pandas
#  Import and read the charity_data.csv.
import pandas as pd 
application_df = pd.read_csv("https://static.bc-edx.com/data/dl-1-2/m21/lms/starter/charity_data.csv")
application_df.head()


application_df.info()


# Example outlier plot of reaction times
times = application_df["ASK_AMT"]
fig1, ax1 = plt.subplots()
ax1.set_title('Reaction Times at Baseball Batting Cage')
ax1.set_ylabel('Reaction Time (ms)')
ax1.boxplot(times)
plt.show()


boxplot = application_df.hvplot.box(y='ASK_AMT', by='IS_SUCCESSFUL', height=400, width=400, legend=True, hover=True)
boxplot


application_df.sort_values(by="ASK_AMT", ascending=False).head(10)


application_df = application_df.drop(axis=0, index=33175)


application_df.info()


# Drop the non-beneficial ID columns, 'EIN' and 'NAME'.
application_df = application_df.drop(columns=["EIN", "NAME"], axis=1)


# Determine the number of unique values in each column.
application_df.nunique()


# Look at APPLICATION_TYPE value counts for binning
application_df["APPLICATION_TYPE"].value_counts()


# Choose a cutoff value and create a list of application types to be replaced
# use the variable name `application_types_to_replace`
application_types_to_replace = ["T9", "T13", "66", "T12", "T2", "T25", "T14", "T29", "T15", "T17"]

# Replace in dataframe
for app in application_types_to_replace:
    application_df['APPLICATION_TYPE'] = application_df['APPLICATION_TYPE'].replace(app,"Other")

# Check to make sure binning was successful
application_df['APPLICATION_TYPE'].value_counts()


# Look at CLASSIFICATION value counts for binning
application_df["CLASSIFICATION"].value_counts()


# You may find it helpful to look at CLASSIFICATION value counts >1
application_df["CLASSIFICATION"].value_counts().loc[lambda x: x>1]


# Choose a cutoff value and create a list of classifications to be replaced
# use the variable name `classifications_to_replace`
# since there are more to replace than to keep, we create a list of values to keep
classifications_to_keep = ["C1000", "C2000", "C1200", "C3000", "C2100"]
classifications_to_replace = application_df[~application_df['CLASSIFICATION'].isin(classifications_to_keep)]
classifications_to_replace = classifications_to_replace['CLASSIFICATION'].value_counts().index.tolist()

# Replace in dataframe
for cls in classifications_to_replace:
    application_df['CLASSIFICATION'] = application_df['CLASSIFICATION'].replace(cls,"Other")
    
# Check to make sure binning was successful
application_df['CLASSIFICATION'].value_counts()


application_df.info()


# Convert categorical data to numeric with `pd.get_dummies`
APPLICATION_TYPE_dummies = pd.get_dummies(application_df['APPLICATION_TYPE'], dtype=int)
AFFILIATION_dummies = pd.get_dummies(application_df['AFFILIATION'], dtype=int)
CLASSIFICATION_dummies = pd.get_dummies(application_df['CLASSIFICATION'], dtype=int)
USE_CASE_dummies = pd.get_dummies(application_df['USE_CASE'], dtype=int)
ORGANIZATION_dummies = pd.get_dummies(application_df['ORGANIZATION'], dtype=int)
INCOME_AMT_dummies = pd.get_dummies(application_df['INCOME_AMT'], dtype=int)
SPECIAL_CONSIDERATIONS_dummies = pd.get_dummies(application_df['SPECIAL_CONSIDERATIONS'], dtype=int)


# create 'numeric_application_df' and drop all categorical columns
numeric_application_df = application_df.drop(columns=['APPLICATION_TYPE', 'AFFILIATION','CLASSIFICATION','USE_CASE','ORGANIZATION','INCOME_AMT','SPECIAL_CONSIDERATIONS'], axis=1)


# concate categorical dummies with application_df
processed_application_df = pd.concat([numeric_application_df,APPLICATION_TYPE_dummies,AFFILIATION_dummies,CLASSIFICATION_dummies,USE_CASE_dummies,ORGANIZATION_dummies,INCOME_AMT_dummies,SPECIAL_CONSIDERATIONS_dummies], axis=1)


# Split our preprocessed data into our features and target arrays
 # features
X = processed_application_df.drop('IS_SUCCESSFUL', axis=1)
 # target
y = processed_application_df['IS_SUCCESSFUL']

# Split the preprocessed data into a training and testing dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)


# Create a StandardScaler instances
scaler = StandardScaler()

# Fit the StandardScaler
X_scaler = scaler.fit(X_train)

# Scale the data
X_train_scaled = X_scaler.transform(X_train)
X_test_scaled = X_scaler.transform(X_test)


X_train_scaled.shape, y_train.shape, X_test_scaled.shape, y_test.shape





# Create a method that creates a new Sequential model with hyperparameter options
def create_model(hp):
    n_hidden = hp.Int("n_hidden", min_value=0, max_value=8, default=2)
    n_neurons = hp.Int("n_neurons", min_value=1, max_value=50)
    learning_rate = hp.Float("learning_rate", min_value=1e-4, max_value=1e-2, sampling="log")
    activation = hp.Choice('activation', values=['relu','tanh'])
    optimizer = hp.Choice("optimizer", values=["sgd","adam"])
    if optimizer == "sgd":
        optimizer = tf.keras.optimizers.legacy.SGD(learning_rate=learning_rate)
    else:
        optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=learning_rate)
        
    model = tf.keras.Sequential()
    
    # Allow kerastuner to decide number of neurons in first layer
    model.add(tf.keras.layers.Dense(units=hp.Int('first_units',
        min_value=1,
        max_value=50,
        step=1), activation=activation, input_dim=43))
    
    # Allow kerastuner to decide number of hidden layers and neurons in hidden layers
    for i in range(n_hidden):
        model.add(tf.keras.layers.Dense(n_neurons, activation=activation))
        
    # Output layer
    model.add(tf.keras.layers.Dense(1, activation="sigmoid"))
    
    model.compile(loss="binary_crossentropy", optimizer=optimizer, metrics=["accuracy"])
    return model


import keras_tuner as kt
random_search_tuner = kt.RandomSearch(
    create_model, objective="val_accuracy", max_trials=10, overwrite=True, directory="ASoup_funding_choice_1", project_name="funding_choice_1", seed=78)

random_search_tuner.search(X_train_scaled,y_train,epochs=100,validation_data=(X_test_scaled,y_test))


top3_hp = random_search_tuner.get_best_hyperparameters(3)
best_hp = top3_hp[0]
best_hp.values


# Evaluate the top model
best_trial = random_search_tuner.oracle.get_best_trials(1)[0]
best_trial.summary()



best_trial.metrics.get_last_value("val_accuracy")


best_model = random_search_tuner.get_best_models(1)[0]
model_loss, model_accuracy = best_model.evaluate(X_test_scaled,y_test,verbose=2)
print(f"Loss: {model_loss}, Accuracy: {model_accuracy}")
# model_loss, model_accuracy = top_model.evaluate(X_test_scaled,y_test,verbose=2)
# print(f"Loss: {model_loss}, Accuracy: {model_accuracy}")


# Export our model to HDF5 file
#  YOUR CODE GOES HERE
