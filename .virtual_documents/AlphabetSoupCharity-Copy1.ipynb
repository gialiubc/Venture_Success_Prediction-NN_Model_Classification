


# Import our dependencies
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import pandas as pd
import tensorflow as tf

#  Import and read the charity_data.csv.
import pandas as pd
application_df = pd.read_csv("https://static.bc-edx.com/data/dl-1-2/m21/lms/starter/charity_data.csv")
application_df.head()


# Drop the non-beneficial ID columns, 'EIN' and 'NAME'.
application_df = application_df.drop(columns=["EIN", "NAME"], axis=1)


# Determine the number of unique values in each column.
application_df.nunique()


# Look at APPLICATION_TYPE value counts for binning
application_df["APPLICATION_TYPE"].value_counts()


# Choose a cutoff value and create a list of application types to be replaced
# use the variable name `application_types_to_replace`
application_types_to_replace = ["T9", "T13", "66", "T12", "T2", "T25", "T14", "T29", "T15", "T17"]

# Replace in dataframe
for app in application_types_to_replace:
    application_df['APPLICATION_TYPE'] = application_df['APPLICATION_TYPE'].replace(app,"Other")

# Check to make sure binning was successful
application_df['APPLICATION_TYPE'].value_counts()


# Look at CLASSIFICATION value counts for binning
application_df["CLASSIFICATION"].value_counts()


# You may find it helpful to look at CLASSIFICATION value counts >1
application_df["CLASSIFICATION"].value_counts().loc[lambda x: x>1]


# Choose a cutoff value and create a list of classifications to be replaced
# use the variable name `classifications_to_replace`
# since there are more to replace than to keep, we create a list of values to keep
classifications_to_keep = ["C1000", "C2000", "C1200", "C3000", "C2100"]
classifications_to_replace = application_df[~application_df['CLASSIFICATION'].isin(classifications_to_keep)]
classifications_to_replace = classifications_to_replace['CLASSIFICATION'].value_counts().index.tolist()

# Replace in dataframe
for cls in classifications_to_replace:
    application_df['CLASSIFICATION'] = application_df['CLASSIFICATION'].replace(cls,"Other")

# Check to make sure binning was successful
application_df['CLASSIFICATION'].value_counts()


application_df.info()


# Convert categorical data to numeric with `pd.get_dummies`
APPLICATION_TYPE_dummies = pd.get_dummies(application_df['APPLICATION_TYPE'], dtype=int)
AFFILIATION_dummies = pd.get_dummies(application_df['AFFILIATION'], dtype=int)
CLASSIFICATION_dummies = pd.get_dummies(application_df['CLASSIFICATION'], dtype=int)
USE_CASE_dummies = pd.get_dummies(application_df['USE_CASE'], dtype=int)
ORGANIZATION_dummies = pd.get_dummies(application_df['ORGANIZATION'], dtype=int)
INCOME_AMT_dummies = pd.get_dummies(application_df['INCOME_AMT'], dtype=int)
SPECIAL_CONSIDERATIONS_dummies = pd.get_dummies(application_df['SPECIAL_CONSIDERATIONS'], dtype=int)


SPECIAL_CONSIDERATIONS_dummies


# create 'numeric_application_df' and drop all categorical columns
numeric_application_df = application_df.drop(columns=['APPLICATION_TYPE', 'AFFILIATION','CLASSIFICATION','USE_CASE','ORGANIZATION','INCOME_AMT','SPECIAL_CONSIDERATIONS'], axis=1)


numeric_application_df


# concate categorical dummies with application_df
processed_application_df = pd.concat([numeric_application_df,APPLICATION_TYPE_dummies,AFFILIATION_dummies,CLASSIFICATION_dummies,USE_CASE_dummies,ORGANIZATION_dummies,INCOME_AMT_dummies,SPECIAL_CONSIDERATIONS_dummies], axis=1)


processed_application_df


# Split our preprocessed data into our features and target arrays
 # features
X = processed_application_df.drop('IS_SUCCESSFUL', axis=1)
 # target
y = processed_application_df['IS_SUCCESSFUL']

# Split the preprocessed data into a training and testing dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)


X.shape


# Create a StandardScaler instances
scaler = StandardScaler()

# Fit the StandardScaler
X_scaler = scaler.fit(X_train)

# Scale the data
X_train_scaled = X_scaler.transform(X_train)
X_test_scaled = X_scaler.transform(X_test)


X_train_scaled.shape





# Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.
#  YOUR CODE GOES HERE

nn = tf.keras.models.Sequential()

# First hidden layer
nn.add(tf.keras.layers.Dense(units=80, activation="relu", input_dim=43))

# Second hidden layer
nn.add(tf.keras.layers.Dense(units=50, activation="relu"))

# Third hidden layer
nn.add(tf.keras.layers.Dense(units=7, activation="relu"))

# Output layer
nn.add(tf.keras.layers.Dense(units=1, activation="sigmoid"))

# Check the structure of the model
nn.summary()


# Compile the model
nn.compile(loss="binary_crossentropy", optimizer="adam", metrics=["accuracy"])


# Train the model
fit_model = nn.fit(X_train_scaled, y_train, epochs=100)



# Evaluate the model using the test data
model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)
print(f"Loss: {model_loss}, Accuracy: {model_accuracy}")


# Export our model to HDF5 file
nn.save('AlphabetSoupCharity_AddLayer.h5')
